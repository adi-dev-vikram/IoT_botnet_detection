{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145c0f0-e2c9-470a-8fb7-b33ab432159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coca\n",
    "\n",
    "# Load the CoCa model\n",
    "model = coca.CoCaModel()\n",
    "\n",
    "# Embed the image\n",
    "image_embedding = model.embed_image(image_path)\n",
    "\n",
    "# Embed the text\n",
    "text_embedding = model.embed_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd254839-9415-45e5-8dc0-113f231ae426",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cocoa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d5fe1-ace5-4d42-af10-0e45dccee498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import imagen\n",
    "\n",
    "# Set the OpenAI API key\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Generate text\n",
    "prompt = f\"Generate text that is related to the following image and text: {image_embedding} {text_embedding}\"\n",
    "response = openai.Completion.create(engine=\"davinci\", prompt=prompt)\n",
    "\n",
    "# Print the generated text\n",
    "print(response.choices[0].text)\n",
    "\n",
    "prompt_text=\"\"\"You are an assistant tasked with summarizing tables, images and text. \\ \n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text) \n",
    "\n",
    "# Summary chain \n",
    "llm_chat = AzureChatOpenAI(temperature=0.0, deployment_name=\"rajatsa-gpt-4\", openai_api_type=\"azure\", openai_api_base=\"https://support-assistant-demo-ai-us-2.openai.azure.com\", openai_api_version=\"2023-08-01-preview\", model_name=\"gpt-4\", openai_api_key=\"2a5292d324ff4947ab6ba9e24746dee1\")  \n",
    "summarize_chain = {\"element\": lambda x:x} | prompt | llm_chat | StrOutputParser()\n",
    "\n",
    "# Load the multimodal transformer model\n",
    "agent = Agent(model_name=\"langchain_multimodal\")\n",
    "# Generate text output\n",
    "text_output = agent.generate(text_input=\"Generate text that is related to the following image:\", image_input=image_path)\n",
    "\n",
    "# Print the generated text\n",
    "print(text_output)\n",
    "\n",
    "\n",
    "\n",
    "# Generate an image\n",
    "image = imagen.diffusion.generate_image(text_output)\n",
    "\n",
    "# Save the image\n",
    "image.save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ee61f-23b7-4e9a-86fc-1d4a48fabf95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
